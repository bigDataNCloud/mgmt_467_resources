{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxutFKw6mOGjHqXoniKykY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Plan: From Historical Analysis to Real-Time Flight Predictions\n",
        "\n",
        "This plan is broken down into the three parts you've outlined. Each part will be a section in the final Colab notebook, with clear instructions and code cells.\n",
        "\n",
        "# **Part I: Batch Processing of Historical Flight Data**\n",
        "\n",
        "Objective: To build a foundational understanding of data engineering and machine learning using a large, static dataset.\n",
        "\n",
        "Cell-by-Cell Plan:\n",
        "\n",
        "Setup and Authentication:\n",
        "\n",
        "Install necessary libraries (google-cloud-storage, google-cloud-bigquery, pandas, db-dtypes).\n",
        "\n",
        "Authenticate the user and configure the GCP project.\n",
        "\n",
        "Define all necessary variables (bucket names, dataset names, etc.).\n",
        "\n",
        "Data Acquisition (Downloading 2024 Flight Data):\n",
        "\n",
        "Provide a function that programmatically downloads the 2024 On-Time Performance data directly from the BTS.gov website. We will select a few months to keep the dataset manageable.\n",
        "\n",
        "The code will download the zipped CSV files and then unzip them.\n",
        "\n",
        "Move Data to GCS Bucket:\n",
        "\n",
        "Create a new GCS bucket.\n",
        "\n",
        "Upload the downloaded and unzipped CSV files to this bucket. This establishes our \"data lake.\"\n",
        "\n",
        "Load Data into a BigQuery Table:\n",
        "\n",
        "Create a new BigQuery dataset.\n",
        "\n",
        "Use a BigQuery Load Job to load all the CSV files from the GCS bucket into a single BigQuery table. We will use schema auto-detection.\n",
        "\n",
        "Data Cleaning and Feature Engineering in BigQuery:\n",
        "\n",
        "Run SQL queries to inspect the data, handle nulls, and create new features. A key feature will be creating a boolean label for logistic regression, such as is_arrival_delayed (TRUE if ARR_DELAY > 15 minutes).\n",
        "\n",
        "Build a Linear Regression Model:\n",
        "\n",
        "Use BigQuery ML to create a linear regression model to predict a continuous variable.\n",
        "\n",
        "Goal: Predict ARR_DELAY based on features like DEP_DELAY, CARRIER, DISTANCE, and DAY_OF_WEEK.\n",
        "\n",
        "Include cells to create, evaluate, and make predictions with the model.\n",
        "\n",
        "Build a Logistic Regression Model:\n",
        "\n",
        "Use BigQuery ML to create a logistic regression model for classification.\n",
        "\n",
        "Goal: Predict the is_arrival_delayed boolean label we created earlier.\n",
        "\n",
        "Include cells to create, evaluate (checking precision/recall), and make predictions.\n",
        "\n",
        "Build a K-Means Clustering Model:\n",
        "\n",
        "Use BigQuery ML to create a K-Means clustering model to segment the data.\n",
        "\n",
        "Goal: Group flights into clusters based on their characteristics (e.g., \"long-haul, often delayed,\" \"short-haul, on-time\").\n",
        "\n",
        "Include cells to create the model and analyze the resulting cluster centroids to understand their meaning.\n",
        "\n",
        "# **Part II: Micro-Batch Processing of \"Live\" Data**\n",
        "\n",
        "Objective: To simulate a streaming environment where new data arrives periodically and build a pipeline to process it.\n",
        "\n",
        "Cell-by-Cell Plan:\n",
        "\n",
        "Introduction to Streaming Concepts:\n",
        "\n",
        "Explain the difference between batch and streaming. Introduce the OpenSky Network API as our source for live flight data.\n",
        "\n",
        "Build a Cloud Function for Data Ingestion (GCS Trigger):\n",
        "\n",
        "Provide the Python code for a Google Cloud Function.\n",
        "\n",
        "Function's Job: This function will be triggered on a schedule (e.g., every 15 minutes using Cloud Scheduler). It will call the OpenSky Network API, fetch all current flight vectors, format the data as a JSON file, and save it to a new GCS bucket (our \"streaming landing zone\").\n",
        "\n",
        "Set up the Pub/Sub and BigQuery Streaming Infrastructure:\n",
        "\n",
        "Create a Pub/Sub topic that will receive notifications about new files.\n",
        "\n",
        "Configure the GCS bucket to send a message to this Pub/Sub topic every time a new file is created.\n",
        "\n",
        "Create a new BigQuery table to hold the streaming data.\n",
        "\n",
        "Build a Dataflow Pipeline (Pub/Sub to BigQuery):\n",
        "\n",
        "Provide the code for a second Cloud Function (or a Dataflow job) that is triggered by messages on the Pub/Sub topic.\n",
        "\n",
        "Function's Job: When triggered, it will read the new JSON file from GCS, parse it, and stream the data into the new BigQuery table.\n",
        "\n",
        "Build ML Models on the Streaming Data:\n",
        "\n",
        "Re-run the same BigQuery ML CREATE MODEL statements from Part I, but this time, train them on the new BigQuery table that is being populated with the streaming data. This demonstrates how models can be updated with fresh data.\n",
        "\n",
        "## **Part III: Real-Time Prediction**\n",
        "\n",
        "Objective: To build a true real-time pipeline that makes predictions on individual events as they happen.\n",
        "\n",
        "Cell-by-Cell Plan:\n",
        "\n",
        "Architecting for Real-Time:\n",
        "\n",
        "Explain why for true real-time, we want to bypass saving files to GCS and send data directly to a message queue.\n",
        "\n",
        "Build a Cloud Function for Direct Streaming (Pub/Sub):\n",
        "\n",
        "Provide the code for a new, optimized Cloud Function.\n",
        "\n",
        "Function's Job: This function will be triggered by a scheduler. It will fetch data from the OpenSky API and publish each flight's data as a separate message directly to a new Pub/Sub topic.\n",
        "\n",
        "Real-Time Prediction Pipeline:\n",
        "\n",
        "Provide the code for a final Cloud Function that acts as our real-time prediction engine.\n",
        "\n",
        "Function's Job: This function will be a subscriber to the Pub/Sub topic from the previous step. For each message (each flight) it receives, it will immediately call the ML.PREDICT function using the models developed in Part II.\n",
        "\n",
        "The prediction results (e.g., \"this flight is likely to be delayed\") can then be saved to a separate \"predictions\" table in BigQuery.\n",
        "\n",
        "Visualizing Real-Time Results:\n",
        "\n",
        "Include a final set of BigQuery queries that students can run to see the predictions populating the new table in near real-time."
      ],
      "metadata": {
        "id": "0qLMfIWGe_hF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmN3sgbvy_w_"
      },
      "outputs": [],
      "source": [
        "# @title ### Cell 1: Setup, Authentication, and Configuration (Corrected)\n",
        "# @markdown **Objective:** This cell imports all necessary Python libraries, authenticates your Google account to allow access to your GCP project, and sets up key configuration variables.\n",
        "\n",
        "# ---\n",
        "# **Libraries Explained:**\n",
        "# - `os`: For interacting with the operating system, like removing files.\n",
        "# - `requests`: A standard library for making HTTP requests to download files from the web.\n",
        "# - `zipfile`: For working with ZIP archives, allowing us to extract the downloaded data.\n",
        "# - `subprocess`: Allows us to run shell commands like `gcloud` and `gsutil` directly from Python.\n",
        "# - `re`: The regular expressions library, used here for cleaning text data.\n",
        "# - `datetime`: For handling dates and looping through the required time period.\n",
        "# - `google.colab.auth`: A specific Colab library to handle authentication with your Google account.\n",
        "# ---\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "from google.colab import auth\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Authenticates the user and configures the necessary GCP project and bucket names.\n",
        "\n",
        "    This function handles the initial setup by authenticating the user's Google account\n",
        "    for use within the Colab environment. It then programmatically determines the\n",
        "    current GCP Project ID. If it cannot be determined automatically, it will prompt\n",
        "    the user to enter it manually. It also prompts the user for a base bucket name\n",
        "    and combines it with the project ID to ensure global uniqueness.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the Project ID and the generated GCS bucket name.\n",
        "               Returns (None, None) if the project ID or bucket name is not provided.\n",
        "    \"\"\"\n",
        "    print(\"Authenticating your Google account...\")\n",
        "    # This command will trigger a pop-up to authenticate your Google account.\n",
        "    auth.authenticate_user()\n",
        "    print(\"‚úÖ Authentication successful.\")\n",
        "\n",
        "    project_id = \"\"\n",
        "    try:\n",
        "        # This command runs the gcloud CLI to get the currently configured project ID.\n",
        "        project_id_process = subprocess.run(\n",
        "            [\"gcloud\", \"config\", \"get-value\", \"project\"],\n",
        "            capture_output=True, text=True, check=True\n",
        "        )\n",
        "        project_id = project_id_process.stdout.strip()\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "        # This block will run if gcloud is not configured or not found.\n",
        "        pass\n",
        "\n",
        "    if not project_id:\n",
        "        print(\"‚ö†Ô∏è Could not automatically determine GCP Project ID.\")\n",
        "        project_id = input(\"Please enter your GCP Project ID: \")\n",
        "\n",
        "    if not project_id:\n",
        "        print(\"üî¥ ERROR: Project ID is required to continue. Halting execution.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"‚úÖ Using GCP Project: {project_id}\")\n",
        "\n",
        "    # --- EDIT: Ask for the bucket name from the user ---\n",
        "    base_bucket_name = input(\"Please enter a base name for your GCS bucket (e.g., 'flight-data'): \")\n",
        "    if not base_bucket_name:\n",
        "        print(\"üî¥ ERROR: A base name for the bucket is required. Halting execution.\")\n",
        "        return project_id, None\n",
        "\n",
        "    # GCS bucket names must be globally unique and cannot contain dots or start with underscores.\n",
        "    # We create a safe, unique name by combining the project ID and the user's input.\n",
        "    safe_project_id = project_id.replace('.', '-')\n",
        "    bucket_name = f\"{safe_project_id}-{base_bucket_name}\"\n",
        "    print(f\"‚úÖ Bucket will be named: {bucket_name}\")\n",
        "    return project_id, bucket_name\n",
        "\n",
        "# Run the setup function and store the variables.\n",
        "PROJECT_ID, BUCKET_NAME = setup_environment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 2: Create GCS Bucket (Corrected)\n",
        "# @markdown **Objective:** This cell checks if the required Google Cloud Storage (GCS) bucket exists, and if not, it creates it.\n",
        "\n",
        "def create_gcs_bucket_if_not_exists(bucket_name, project_id):\n",
        "    \"\"\"\n",
        "    Checks for the existence of a GCS bucket and creates it if it's not found.\n",
        "\n",
        "    This function uses the `gsutil` command-line tool to interact with GCS.\n",
        "    It first tries to list the contents of the target bucket. If this command\n",
        "    fails, it assumes the bucket does not exist and proceeds to create it\n",
        "    using `gsutil mb` (make bucket).\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The name of the GCS bucket to check and create.\n",
        "        project_id (str): The GCP project ID to associate with the bucket creation.\n",
        "    \"\"\"\n",
        "    if not bucket_name or not project_id:\n",
        "        print(\"üî¥ ERROR: Bucket name or Project ID is not set. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nChecking for GCS bucket: gs://{bucket_name}\")\n",
        "    try:\n",
        "        # The `gsutil ls` command will fail if the bucket does not exist.\n",
        "        subprocess.run([\"gsutil\", \"ls\", f\"gs://{bucket_name}\"], check=True, capture_output=True)\n",
        "        print(f\"‚úÖ Bucket gs://{bucket_name} already exists.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Bucket not found. Creating gs://{bucket_name}...\")\n",
        "        # `gsutil mb` creates a new bucket.\n",
        "        try:\n",
        "            # --- FIX: Added the '-p [project_id]' flag to the gsutil command ---\n",
        "            subprocess.run(\n",
        "                [\"gsutil\", \"mb\", \"-p\", project_id, f\"gs://{bucket_name}\"],\n",
        "                check=True, capture_output=True, text=True\n",
        "            )\n",
        "            print(f\"‚úÖ Bucket gs://{bucket_name} created.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"üî¥ ERROR: Failed to create bucket. The name may be taken or invalid.\")\n",
        "            print(f\"   Details: {e.stderr}\")\n",
        "\n",
        "\n",
        "# Run the bucket creation function using the globally defined variables.\n",
        "create_gcs_bucket_if_not_exists(BUCKET_NAME, PROJECT_ID)"
      ],
      "metadata": {
        "id": "wjZSrRdH0zvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 3: Define the Data Download and Preparation Function\n",
        "# @markdown **Objective:** This cell defines the main worker function, `download_and_prepare_month`, which handles the entire ETL (Extract, Transform, Load) process for a single month's data.\n",
        "\n",
        "def download_and_prepare_month(year, month, bucket_name):\n",
        "    \"\"\"\n",
        "    Downloads, unzips, cleans, and uploads data for a specific month and year.\n",
        "\n",
        "    This function encapsulates the logic from the shell script's `prepareMonthData`\n",
        "    function, performing the following steps:\n",
        "    1.  Constructs the download URL and downloads the ZIP file using `requests`.\n",
        "    2.  Extracts the contents of the ZIP file using `zipfile`.\n",
        "    3.  Finds the specific CSV file for the month and cleans it by removing\n",
        "        trailing commas and all quotation marks, replicating the `sed` commands.\n",
        "    4.  Uploads the cleaned CSV file to a specific folder within the GCS bucket.\n",
        "    5.  Cleans up all temporary local files.\n",
        "\n",
        "    Args:\n",
        "        year (int): The year of the data to download.\n",
        "        month (int): The month of the data to download.\n",
        "        bucket_name (str): The GCS bucket to upload the final data to.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Preparing data for {month:02d}/{year} ---\")\n",
        "\n",
        "    # 1. Download the data file from BTS.gov\n",
        "    print(\"  Downloading...\")\n",
        "    url = f\"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip\"\n",
        "    downloaded_zip_file = f\"{year}-{month}.zip\"\n",
        "\n",
        "    try:\n",
        "        # We use requests.get() to download the file.\n",
        "        # `verify=False` is used to bypass potential SSL certificate issues, similar to `curl -k`.\n",
        "        response = requests.get(url, verify=False, timeout=300)\n",
        "        response.raise_for_status()  # This will raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(downloaded_zip_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  üî¥ ERROR: Cannot download data for {month}/{year}. URL may be invalid or server is down. {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Unzip the downloaded file\n",
        "    print(\"  Unzipping...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(downloaded_zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"  üî¥ ERROR: Failed to unzip file for {month}/{year}. It may not be a valid zip archive.\")\n",
        "        if os.path.exists(downloaded_zip_file):\n",
        "            os.remove(downloaded_zip_file)\n",
        "        return\n",
        "\n",
        "    # 3. Clean the extracted CSV data\n",
        "    print(\"  Cleaning...\")\n",
        "    cleaned_csv_file = f\"{year}-{month}.csv\"\n",
        "    original_csv_name = None\n",
        "    # Find the CSV file that was just unzipped.\n",
        "    for file in os.listdir(\".\"):\n",
        "        if file.endswith(f\"_{year}_{month}.csv\"):\n",
        "            original_csv_name = file\n",
        "            break\n",
        "\n",
        "    if not original_csv_name:\n",
        "        print(f\"  üî¥ ERROR: No CSV file found after unzipping for {month}/{year}.\")\n",
        "        if os.path.exists(downloaded_zip_file): os.remove(downloaded_zip_file)\n",
        "        return\n",
        "\n",
        "    # Read the original file, clean each line, and write to a new file.\n",
        "    with open(original_csv_name, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
        "         open(cleaned_csv_file, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            # Replicates `sed -e 's/,$//g' -e 's/\"//g'`\n",
        "            # 1. Remove trailing comma if it exists.\n",
        "            # 2. Remove all quotation marks.\n",
        "            cleaned_line = re.sub(r',?$', '', line.strip()).replace('\"', '')\n",
        "            outfile.write(cleaned_line + '\\n')\n",
        "\n",
        "    # 4. Upload the cleaned file to Google Cloud Storage\n",
        "    print(\"  Storing in GCS...\")\n",
        "    destination_blob_name = f\"data/flightsETL/{cleaned_csv_file}\"\n",
        "    try:\n",
        "        # Use `gsutil cp` to copy the local file to the GCS bucket.\n",
        "        subprocess.run(\n",
        "            [\"gsutil\", \"cp\", cleaned_csv_file, f\"gs://{bucket_name}/{destination_blob_name}\"],\n",
        "            check=True, capture_output=True\n",
        "        )\n",
        "        print(f\"  ‚úÖ Successfully uploaded {cleaned_csv_file}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"  üî¥ ERROR: Failed to upload to GCS. {e.stderr.decode()}\")\n",
        "\n",
        "    # 5. Clean up all local files to save space\n",
        "    print(\"  Cleaning up local files...\")\n",
        "    if os.path.exists(downloaded_zip_file): os.remove(downloaded_zip_file)\n",
        "    if os.path.exists(original_csv_name): os.remove(original_csv_name)\n",
        "    if os.path.exists(cleaned_csv_file): os.remove(cleaned_csv_file)\n",
        "    # Also remove any other potential extracted files like readme.html\n",
        "    if os.path.exists(\"readme.html\"): os.remove(\"readme.html\")"
      ],
      "metadata": {
        "id": "m4l4_vw8YAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 4: Main Execution Loop\n",
        "# @markdown **Objective:** This cell defines the date range for the data download and then loops through each month, calling the worker function defined in the previous cell.\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"\n",
        "    Manages the overall execution flow. It sets the date range and iterates\n",
        "    through each month, calling the data preparation function.\n",
        "    \"\"\"\n",
        "    if not BUCKET_NAME:\n",
        "        print(\"üî¥ ERROR: BUCKET_NAME is not defined. Halting execution.\")\n",
        "        return\n",
        "\n",
        "    # --- Define Date Range ---\n",
        "    # You can change these values to download a different range of data.\n",
        "    start_year, start_month = 2024, 1  # Start: January 2024\n",
        "    end_year, end_month = 2024, 3      # End: March 2024\n",
        "\n",
        "    print(f\"Starting pipeline to download data from {start_month}/{start_year} to {end_month}/{end_year}.\")\n",
        "\n",
        "    # --- Loop Through Dates ---\n",
        "    current_date = datetime(start_year, start_month, 1)\n",
        "    end_date = datetime(end_year, end_month, 1)\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        download_and_prepare_month(current_date.year, current_date.month, BUCKET_NAME)\n",
        "\n",
        "        # Increment the date to the next month\n",
        "        # This logic correctly handles rolling over to the next year.\n",
        "        next_month = current_date.month + 1\n",
        "        next_year = current_date.year\n",
        "        if next_month > 12:\n",
        "            next_month = 1\n",
        "            next_year += 1\n",
        "        current_date = datetime(next_year, next_month, 1)\n",
        "\n",
        "    print(\"\\n--- Pipeline Execution Complete ---\")\n",
        "\n",
        "# Execute the main pipeline function.\n",
        "run_pipeline()"
      ],
      "metadata": {
        "id": "YKzCAkdBYK5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 5: Final Verification\n",
        "# @markdown **Objective:** This final cell runs the `gsutil ls -l` command to list the contents of the target GCS directory. This allows you to verify that all the cleaned CSV files were successfully uploaded.\n",
        "\n",
        "def verify_uploads(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists the final contents of the target GCS directory to verify uploads.\n",
        "    \"\"\"\n",
        "    if not bucket_name:\n",
        "        print(\"üî¥ ERROR: BUCKET_NAME is not defined. Cannot verify.\")\n",
        "        return\n",
        "\n",
        "    target_directory = f\"gs://{bucket_name}/data/flightsETL/\"\n",
        "    print(f\"\\nVerifying final contents of {target_directory}...\")\n",
        "\n",
        "    try:\n",
        "        # Use subprocess to run the gsutil command and capture its output.\n",
        "        result = subprocess.run(\n",
        "            [\"gsutil\", \"ls\", \"-l\", target_directory],\n",
        "            check=True, capture_output=True, text=True\n",
        "        )\n",
        "        print(\"‚úÖ Uploads confirmed:\")\n",
        "        print(result.stdout)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"üî¥ ERROR: Could not list directory contents. It may be empty or there was an error.\")\n",
        "        print(e.stderr)\n",
        "\n",
        "# Run the verification function.\n",
        "verify_uploads(BUCKET_NAME)"
      ],
      "metadata": {
        "id": "Irajc0g_YROA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 6: Create BigQuery Dataset\n",
        "# @markdown **Objective:** This cell creates a new dataset in BigQuery to store our flight data. A dataset is a container for your tables, similar to a schema in a traditional database.\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import Conflict\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Define the name for your new BigQuery dataset\n",
        "BIGQUERY_DATASET = \"flights_data\"\n",
        "dataset_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}\"\n",
        "\n",
        "try:\n",
        "    # Create a Dataset object\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    # Specify the location for the dataset\n",
        "    dataset.location = \"US\" # You can change this to your preferred location\n",
        "    # Make an API request to create the dataset\n",
        "    client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"‚úÖ Successfully created dataset: {dataset_id}\")\n",
        "except Conflict:\n",
        "    print(f\"‚úÖ Dataset '{dataset_id}' already exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"üî¥ An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Gon0xv6UhWGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 7: Load All CSVs from GCS into a BigQuery Table\n",
        "# @markdown **Objective:** This cell uses a BigQuery Load Job to efficiently load all the cleaned CSV files from your GCS bucket into a single BigQuery table. This is the recommended method for batch loading from GCS.\n",
        "\n",
        "# Define the name for the new table\n",
        "BIGQUERY_TABLE = \"flights_raw\"\n",
        "table_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}\"\n",
        "\n",
        "# Configure the Load Job\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Automatically infer the schema from the data.\n",
        "    autodetect=True,\n",
        "    # Skip the first row of each file, which contains the headers.\n",
        "    skip_leading_rows=1,\n",
        "    # The source format is CSV.\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    # Allow for rows that might have too few columns.\n",
        "    allow_jagged_rows=True,\n",
        "    # Overwrite the table if it already exists.\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        ")\n",
        "\n",
        "# Define the GCS URI using a wildcard to select all CSV files in the folder\n",
        "uri = f\"gs://{BUCKET_NAME}/data/flightsETL/*.csv\"\n",
        "\n",
        "try:\n",
        "    # Start the Load Job\n",
        "    load_job = client.load_table_from_uri(\n",
        "        uri, table_id, job_config=job_config\n",
        "    )\n",
        "    print(f\"üöÄ Starting BigQuery load job {load_job.job_id}...\")\n",
        "\n",
        "    # Waits for the job to complete.\n",
        "    load_job.result()\n",
        "    print(\"‚úÖ Load job finished.\")\n",
        "\n",
        "    # Get the destination table object and print the row count\n",
        "    destination_table = client.get_table(table_id)\n",
        "    print(f\"Loaded {destination_table.num_rows} rows into table '{BIGQUERY_TABLE}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"üî¥ An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "b9tRl4Shhexx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 8: Data Cleaning and Feature Engineering (Corrected)\n",
        "# @markdown **Objective:** This cell runs a SQL query to create a new, cleaned view of our data. This view will serve as the basis for our machine learning models. We will create a key boolean label, `is_arrival_delayed`, for our classification model.\n",
        "\n",
        "# Define the name for our new view\n",
        "CLEANED_VIEW = \"flights_cleaned\"\n",
        "view_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.{CLEANED_VIEW}\"\n",
        "\n",
        "# This SQL query selects relevant columns and creates our new feature.\n",
        "# A VIEW is a virtual table based on the result-set of an SQL statement.\n",
        "# It's a great way to create a clean dataset without duplicating data.\n",
        "# --- FIX: Using the correct column names from the provided schema ---\n",
        "sql_query = f\"\"\"\n",
        "CREATE OR REPLACE VIEW `{view_id}` AS\n",
        "SELECT\n",
        "  -- Construct a DATE type from Year, Month, DayofMonth columns\n",
        "  PARSE_DATE('%Y%m%d', CONCAT(CAST(Year AS STRING), LPAD(CAST(Month AS STRING), 2, '0'), LPAD(CAST(DayofMonth AS STRING), 2, '0'))) AS FL_DATE,\n",
        "  IATA_CODE_Reporting_Airline AS CARRIER,\n",
        "  Origin,\n",
        "  Dest,\n",
        "  DepDelay,\n",
        "  ArrDelay,\n",
        "  Distance,\n",
        "  CAST(DayOfWeek AS STRING) AS DAY_OF_WEEK,\n",
        "  -- Create a boolean label: TRUE if arrival delay is > 15 mins, FALSE otherwise.\n",
        "  -- We also treat NULL delays as not delayed.\n",
        "  CASE\n",
        "    WHEN ArrDelay > 15 THEN TRUE\n",
        "    ELSE FALSE\n",
        "  END AS is_arrival_delayed\n",
        "FROM\n",
        "  `{table_id}`\n",
        "WHERE\n",
        "  -- Filter out cancelled and diverted flights for accurate modeling\n",
        "  Cancelled = 0 AND Diverted = 0;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Execute the query to create the view\n",
        "    query_job = client.query(sql_query)\n",
        "    query_job.result() # Wait for the job to complete\n",
        "    print(f\"‚úÖ Successfully created cleaned view: {CLEANED_VIEW}\")\n",
        "\n",
        "    # Verify by showing the first 10 rows of the new view\n",
        "    print(\"\\n--- Sample of Cleaned Data ---\")\n",
        "    df = client.query(f\"SELECT * FROM `{view_id}` LIMIT 10\").to_dataframe()\n",
        "    display(df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"üî¥ An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "MBYom5ZpiOXx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 9: Build, Evaluate, and Predict with a Linear Regression Model\n",
        "# @markdown **Objective:** Use BigQuery ML to create a linear regression model to predict the arrival delay (`ARR_DELAY`).\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound # Import NotFound for specific error handling\n",
        "\n",
        "# Re-initialize the BigQuery client to ensure it's fresh and correctly scoped\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Re-define necessary variables as they might have been cleared\n",
        "CLEANED_VIEW = \"flights_cleaned\"\n",
        "view_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.{CLEANED_VIEW}\"\n",
        "\n",
        "print(f\"Attempting to use view: {view_id}\")\n",
        "\n",
        "# --- Add a check for view existence before proceeding ---\n",
        "try:\n",
        "    client.get_table(view_id) # get_table works for views too\n",
        "    print(f\"‚úÖ BigQuery view '{view_id}' confirmed to exist.\")\n",
        "except NotFound:\n",
        "    print(f\"üî¥ ERROR: BigQuery view '{view_id}' not found. Please ensure Cell 8 (Data Cleaning and Feature Engineering) ran successfully and created the view.\")\n",
        "    raise # Re-raise the error to stop execution if the view is truly not found\n",
        "except Exception as e:\n",
        "    print(f\"üî¥ An unexpected error occurred while checking view existence: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 1. Create the Linear Regression Model ---\n",
        "print(\"üöÄ Training Linear Regression model...\")\n",
        "create_linear_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_predictor`\n",
        "OPTIONS(model_type='LINEAR_REG', input_label_cols=['ArrDelay']) AS\n",
        "SELECT\n",
        "  ArrDelay,\n",
        "  DepDelay,\n",
        "  CARRIER,\n",
        "  Distance,\n",
        "  DAY_OF_WEEK\n",
        "FROM\n",
        "  `{view_id}`;\n",
        "\"\"\"\n",
        "linear_job = client.query(create_linear_model_query)\n",
        "linear_job.result()\n",
        "print(\"‚úÖ Linear Regression model created successfully.\")\n",
        "\n",
        "# --- 2. Evaluate the Model ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "evaluate_linear_model_query = f\"\"\"\n",
        "SELECT * FROM ML.EVALUATE(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_predictor`);\n",
        "\"\"\"\n",
        "linear_eval_df = client.query(evaluate_linear_model_query).to_dataframe()\n",
        "display(linear_eval_df)\n",
        "\n",
        "# --- 3. Make Predictions with the Model ---\n",
        "print(\"\\n--- Sample Predictions ---\")\n",
        "predict_linear_query = f\"\"\"\n",
        "SELECT\n",
        "  ArrDelay AS actual_delay,\n",
        "  predicted_ArrDelay\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_predictor`,\n",
        "    (SELECT * FROM `{view_id}` LIMIT 10));\n",
        "\"\"\"\n",
        "linear_predict_df = client.query(predict_linear_query).to_dataframe()\n",
        "display(linear_predict_df)"
      ],
      "metadata": {
        "id": "AW6_4VZVoV4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 10: Build, Evaluate, and Predict with a Logistic Regression Model\n",
        "# @markdown **Objective:** Use BigQuery ML to create a logistic regression model to predict if a flight will be significantly delayed (`is_arrival_delayed`).\n",
        "\n",
        "# --- 1. Create the Logistic Regression Model ---\n",
        "print(\"üöÄ Training Logistic Regression model...\")\n",
        "create_logistic_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_classifier`\n",
        "OPTIONS(model_type='LOGISTIC_REG', input_label_cols=['is_arrival_delayed']) AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{view_id}`;\n",
        "\"\"\"\n",
        "logistic_job = client.query(create_logistic_model_query)\n",
        "logistic_job.result()\n",
        "print(\"‚úÖ Logistic Regression model created successfully.\")\n",
        "\n",
        "# --- 2. Evaluate the Model ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "evaluate_logistic_model_query = f\"\"\"\n",
        "SELECT * FROM ML.EVALUATE(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_classifier`);\n",
        "\"\"\"\n",
        "logistic_eval_df = client.query(evaluate_logistic_model_query).to_dataframe()\n",
        "display(logistic_eval_df)\n",
        "\n",
        "# --- 3. Make Predictions with the Model ---\n",
        "print(\"\\n--- Sample Predictions ---\")\n",
        "# --- FIX: Changed 'prob' to the correct column name 'predicted_is_arrival_delayed_probs' ---\n",
        "predict_logistic_query = f\"\"\"\n",
        "SELECT\n",
        "  is_arrival_delayed AS actual_is_delayed,\n",
        "  predicted_is_arrival_delayed,\n",
        "  predicted_is_arrival_delayed_probs\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_delay_classifier`,\n",
        "    (SELECT * FROM `{view_id}` LIMIT 10))\n",
        "\"\"\"\n",
        "logistic_predict_df = client.query(predict_logistic_query).to_dataframe()\n",
        "display(logistic_predict_df)"
      ],
      "metadata": {
        "id": "dcNfV9ekos-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 11: Build and Analyze a K-Means Clustering Model\n",
        "# @markdown **Objective:** Use BigQuery ML's K-Means to group flights into distinct clusters based on their characteristics.\n",
        "\n",
        "# --- 1. Create the K-Means Clustering Model ---\n",
        "print(\"üöÄ Training K-Means Clustering model...\")\n",
        "create_kmeans_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_clusterer`\n",
        "OPTIONS(model_type='KMEANS', num_clusters=5) AS\n",
        "SELECT\n",
        "  DepDelay,\n",
        "  ArrDelay,\n",
        "  Distance\n",
        "FROM\n",
        "  `{view_id}`\n",
        "WHERE DepDelay IS NOT NULL AND ArrDelay IS NOT NULL;\n",
        "\"\"\"\n",
        "kmeans_job = client.query(create_kmeans_model_query)\n",
        "kmeans_job.result()\n",
        "print(\"‚úÖ K-Means Clustering model created successfully.\")\n",
        "\n",
        "# --- 2. Analyze the Cluster Centroids ---\n",
        "print(\"\\n--- Cluster Centroid Analysis ---\")\n",
        "# ML.CENTROIDS shows the average values for each feature within each cluster.\n",
        "# This helps us understand what defines each cluster (e.g., Cluster 1 is short-haul flights with minor delays).\n",
        "analyze_centroids_query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.CENTROIDS(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_clusterer`);\n",
        "\"\"\"\n",
        "centroids_df = client.query(analyze_centroids_query).to_dataframe()\n",
        "display(centroids_df)\n",
        "\n",
        "# --- 3. See which Cluster Each Flight Belongs To ---\n",
        "print(\"\\n--- Sample Cluster Assignments ---\")\n",
        "predict_kmeans_query = f\"\"\"\n",
        "SELECT\n",
        "  CENTROID_ID,\n",
        "  DepDelay,\n",
        "  ArrDelay,\n",
        "  Distance\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{PROJECT_ID}.{BIGQUERY_DATASET}.flight_clusterer`,\n",
        "    (SELECT * FROM `{view_id}` LIMIT 10));\n",
        "\"\"\"\n",
        "kmeans_predict_df = client.query(predict_kmeans_query).to_dataframe()\n",
        "display(kmeans_predict_df)"
      ],
      "metadata": {
        "id": "PgBj8MJJpMbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}